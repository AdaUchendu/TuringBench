{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BertAA for TuringBench",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdaUchendu/TuringBench/blob/master/BertAA_for_TuringBench.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92mikRZyvyje"
      },
      "source": [
        "# **BertAA: BERT fine-tuning for Authorship Attribution**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Paper**: https://www.overleaf.com/6941722415tfbcbyzmxvsh\n",
        "\n",
        "---\n",
        "\n",
        "Authors: Maël Fabien, Esaú Villatoro-Tello, Petr Motlicek, Shantipriya Parida\n",
        "\n",
        "*Idiap Research Institute, Martigny, Switzerland*\n",
        "\n",
        "*Ecole Polytechnique Fédérale de Lausanne (EPFL), Switzerland*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2tOGQ0AiX3q"
      },
      "source": [
        "# I. Task description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDYVDF74A-iJ"
      },
      "source": [
        "## Main idea:\n",
        "- Train an authorship attribution algorithm on Enron E-mail corpus, IMDB movie reviews and Blog Authroship corpus\n",
        "- Compare several approaches:\n",
        "  - TF-IDF + Classifier\n",
        "  - Manual feature extraction + Classifier\n",
        "  - Pre-trained Bert + Classifier\n",
        "  - Additional layer on pre-trained Bert\n",
        "- Compare the performance on 5, 10, 25, 50, 75 and 100 characters\n",
        "\n",
        "Datasets can be downloaded here:\n",
        "- IMDb and IMDb 62: https://umlt.infotech.monash.edu/?page_id=266\n",
        "- Blog: https://www.kaggle.com/rtatman/blog-authorship-corpus/data#\n",
        "- Enron: Preparation done by LUH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLMFEBrJPuEP"
      },
      "source": [
        "## Papers on Enron - Summary:\n",
        "\n",
        "| Paper | Approach | Result | Year|\n",
        "| --- | --- | --- |--- |\n",
        "|[An Exploratory Study on Authorship Verification Models for Forensic Purpose](https://repository.tudelft.nl/islandora/object/uuid:f43d489f-c53c-4392-824b-e1a8eaebb827/datastream/OBJ/download) | Master thesis - N-Gram (word & char) - 38 authors| Best Accuracy: 0.73 | 2013 |\n",
        "| [Authorship Attribution on the Enron Email Corpus](https://dsc.duq.edu/cgi/viewcontent.cgi?article=1839&context=etd) | Master Thesis - N-Gram (word & char) - Several author sizes | Best Accuracy: 0.5353| 2013 |\n",
        "| [Authorship Attribution of E-Mail: Comparing Classifiers Over a New Corpus for Evaluation](http://www.lrec-conf.org/proceedings/lrec2008/pdf/552_paper.pdf) | N-gram (Word), stemming - Hierarchical Prob. Clf - 9 authors| Best Accuracy: 0.8705| 2008 |\n",
        "| [Authorship Verification for Short Messages using Stylometry](https://www.uvic.ca/engineering/ece/isot/assets/docs/Authorship_Verification_for_Short_Messages_using_Stylometry.pdf) | N-gram (absent, present, not freq) - 87 authors| EER 14.35% |2013 |\n",
        "| [E-Mail Authorship Attribution applied to the Extended Enron Authorship Corpus (XEAC)](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.466.4644&rep=rep1&type=pdf) | Custom style-feature extraction| Best accuracy: 0.8922 | 2009|\n",
        "| [Identifying idiolect in forensic authorship attribution: an n-gram textbite approach](http://irep.ntu.ac.uk/id/eprint/15374/1/220199_PubSub2404_Wright.pdf) | 1 author vs. rest - N-Grams| Best accuracy: 100% (Not same task) | |\n",
        "| [CEAI: CCM-based email authorship identification model](https://www.sciencedirect.com/science/article/pii/S111086651300039X) | Stylometric + Content features | 94% - 10 aut., 89% - 25 aut., 81% - 50 aut.| 2013 |\n",
        "\n",
        "## Papers on techniques (Not Enron) - Summary:\n",
        "\n",
        "| Paper | Approach | Result | Year|\n",
        "| --- | --- | --- |--- |\n",
        "| [Siamese Networks for Large-Scale Author Identification](https://www.groundai.com/project/siamese-networks-for-large-scale-author-identification/1) | Siamese networks based on Bert | 94% - 2 authors, 40% - 50 authors | 2019 |\n",
        "| [Topic or Style? Exploring the Most Useful Features for Authorship Attribution](https://www.aclweb.org/anthology/C18-1029.pdf) | Feature extraction process | |2018 |\n",
        "| [Learning Invariant Representations of Social Media Users](https://www.aclweb.org/anthology/D19-1178.pdf) | Learn Embeddings with an Angular Margin | Works for unseen users (similarity between user styles) | 2019 |\n",
        "| [Authorship Attribution for Forensic Investigation with Thousands of Authors](https://link.springer.com/content/pdf/10.1007%2F978-3-642-55415-5_28.pdf) | Manual features + gender and age classifier | Reaching 39% on thousands of authors | 2014 |\n",
        "| [Convolutional Neural Networks for Authorship Attribution of Short Texts](https://www.aclweb.org/anthology/E17-2106.pdf) | Character N-gram + CNN + Softmax | 50% on 100 tweet authors | 2017 |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Other information\n",
        "\n",
        "- List of potential conferences to publish to: http://www.wikicfp.com/cfp/call?conference=NLP\n",
        "\n",
        "- **Dataset**: the dataset has been already saved on Google Drive, and is available publically using the lines of code provided below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jji3r4lGj1im"
      },
      "source": [
        "**NB**: Imports due to some installs take on average 10 minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHnVnXMJDsmC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "026e8398-3b33-481d-edff-f39cc2d0ab91"
      },
      "source": [
        "# General\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import itertools\n",
        "from pandas import DataFrame\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.font_manager as fm\n",
        "from matplotlib.collections import QuadMesh\n",
        "import seaborn as sn\n",
        "import plotly.express as px\n",
        "\n",
        "# Feature extraction approach\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from string import punctuation\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.tokenize import word_tokenize \n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Classification\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgbm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# BERT classifier\n",
        "# Installing a custom version of Simple Transformers\n",
        "!git clone https://github.com/NVIDIA/apex\n",
        "!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex\n",
        "#!git init\n",
        "!pip install --upgrade tqdm\n",
        "#!git remote add origin https://github.com/ThilinaRajapakse/simpletransformers.git\n",
        "#!git pull origin master\n",
        "##!pip install -r requirements-dev.txt\n",
        "!pip install transformers\n",
        "!pip install tensorboardX\n",
        "\n",
        "!pip install simpletransformers\n",
        "from simpletransformers.classification import ClassificationModel\n",
        "\n",
        "import torch\n",
        "\n",
        "# Access to the file\n",
        "!pip install PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Parallelize apply on Pandas\n",
        "!pip install pandarallel\n",
        "from pandarallel import pandarallel\n",
        "pandarallel.initialize()\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "fatal: destination path 'apex' already exists and is not an empty directory.\n",
            "/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py:283: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n",
            "  cmdoptions.check_install_build_global(options)\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-l6_aq1eu\n",
            "Created temporary directory: /tmp/pip-req-tracker-pk2p8oz3\n",
            "Created requirements tracker '/tmp/pip-req-tracker-pk2p8oz3'\n",
            "Created temporary directory: /tmp/pip-install-a9y6adtd\n",
            "Processing ./apex\n",
            "  Created temporary directory: /tmp/pip-req-build-9hsqwett\n",
            "  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-pk2p8oz3'\n",
            "    Running setup.py (path:/tmp/pip-req-build-9hsqwett/setup.py) egg_info for package from file:///content/apex\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.8.1+cu101\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-req-build-9hsqwett/pip-egg-info/apex.egg-info\n",
            "    writing /tmp/pip-req-build-9hsqwett/pip-egg-info/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-req-build-9hsqwett/pip-egg-info/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-req-build-9hsqwett/pip-egg-info/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-req-build-9hsqwett/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    adding license file 'LICENSE' (matched pattern 'LICEN[CS]E*')\n",
            "    writing manifest file '/tmp/pip-req-build-9hsqwett/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    /tmp/pip-req-build-9hsqwett/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  Source in /tmp/pip-req-build-9hsqwett has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n",
            "  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-pk2p8oz3'\n",
            "Skipping wheel build for apex, due to binaries being disabled for it.\n",
            "Installing collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-record-w7azbxsq\n",
            "    Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-9hsqwett/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-9hsqwett/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-w7azbxsq/install-record.txt --single-version-externally-managed --compile\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.8.1+cu101\n",
            "\n",
            "\n",
            "    /tmp/pip-req-build-9hsqwett/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "\n",
            "    Compiling cuda extensions with\n",
            "    nvcc: NVIDIA (R) Cuda compiler driver\n",
            "    Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "    Built on Wed_Jul_22_19:09:09_PDT_2020\n",
            "    Cuda compilation tools, release 11.0, V11.0.221\n",
            "    Build cuda_11.0_bu.TC445_37.28845127_0\n",
            "    from /usr/local/cuda/bin\n",
            "\n",
            "    Traceback (most recent call last):\n",
            "      File \"<string>\", line 1, in <module>\n",
            "      File \"/tmp/pip-req-build-9hsqwett/setup.py\", line 171, in <module>\n",
            "        check_cuda_torch_binary_vs_bare_metal(torch.utils.cpp_extension.CUDA_HOME)\n",
            "      File \"/tmp/pip-req-build-9hsqwett/setup.py\", line 106, in check_cuda_torch_binary_vs_bare_metal\n",
            "        \"https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  \"\n",
            "    RuntimeError: Cuda extensions are being compiled with a version of Cuda that does not match the version used to compile Pytorch binaries.  Pytorch binaries were compiled with Cuda 10.1.\n",
            "    In some cases, a minor-version mismatch will not cause later errors:  https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  You can try commenting out this check (at your own risk).\n",
            "    Running setup.py install for apex ... \u001b[?25l\u001b[?25herror\n",
            "Cleaning up...\n",
            "  Removing source in /tmp/pip-req-build-9hsqwett\n",
            "Removed build tracker '/tmp/pip-req-tracker-pk2p8oz3'\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-9hsqwett/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-9hsqwett/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-w7azbxsq/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n",
            "Exception information:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 153, in _main\n",
            "    status = self.run(options, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 455, in run\n",
            "    use_user_site=options.use_user_site,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/req/__init__.py\", line 62, in install_given_reqs\n",
            "    **kwargs\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/req/req_install.py\", line 888, in install\n",
            "    cwd=self.unpacked_source_directory,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/utils/subprocess.py\", line 275, in runner\n",
            "    spinner=spinner,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/utils/subprocess.py\", line 242, in call_subprocess\n",
            "    raise InstallationError(exc_msg)\n",
            "pip._internal.exceptions.InstallationError: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-9hsqwett/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-9hsqwett/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-w7azbxsq/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\n",
            "Requirement already up-to-date: tqdm in /usr/local/lib/python3.7/dist-packages (4.60.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.60.0)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.7/dist-packages (2.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (56.1.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: simpletransformers in /usr/local/lib/python3.7/dist-packages (0.61.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (2.23.0)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.2.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (0.1.95)\n",
            "Requirement already satisfied: tensorboardx in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (2.2)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (0.10.30)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.19.5)\n",
            "Requirement already satisfied: transformers>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (4.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.4.1)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (0.82.0)\n",
            "Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (4.60.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (0.10.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.1.5)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.6.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (2019.12.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (1.24.3)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardx->simpletransformers) (3.12.4)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers) (2.8.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers) (3.13)\n",
            "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers) (1.1.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers) (5.4.8)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers) (3.5.4)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers) (5.0.2)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers) (0.1.2)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers) (3.1.17)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers) (2.3)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers) (7.1.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers) (1.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.2.0->simpletransformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=4.2.0->simpletransformers) (0.0.45)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.2.0->simpletransformers) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers>=4.2.0->simpletransformers) (4.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=4.2.0->simpletransformers) (20.9)\n",
            "Requirement already satisfied: pydeck>=0.1.dev5 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (0.6.2)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (0.10.2)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (0.18.2)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (0.8.1)\n",
            "Requirement already satisfied: watchdog; platform_system != \"Darwin\" in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (2.1.1)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (1.5.1)\n",
            "Requirement already satisfied: base58 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (2.1.0)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (5.1.1)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (4.1.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (7.1.2)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (4.2.2)\n",
            "Requirement already satisfied: pyarrow; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (3.0.0)\n",
            "Requirement already satisfied: blinker in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (1.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->simpletransformers) (1.0.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->simpletransformers) (2018.9)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (0.70.11.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (0.3.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (2.0.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (2021.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardx->simpletransformers) (56.1.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb->simpletransformers) (4.0.7)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb->simpletransformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers>=4.2.0->simpletransformers) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers>=4.2.0->simpletransformers) (2.4.7)\n",
            "Requirement already satisfied: ipykernel>=5.1.2; python_version >= \"3.4\" in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (5.5.5)\n",
            "Requirement already satisfied: jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (2.11.3)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (7.6.3)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (5.0.5)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators->streamlit->simpletransformers) (4.4.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.11.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (2.6.0)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb->simpletransformers) (4.0.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.5.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.3.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.10.1->pydeck>=0.1.dev5->streamlit->simpletransformers) (2.0.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.1.3)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.0.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (3.5.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.3.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->simpletransformers) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.5)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.7.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->simpletransformers) (22.0.3)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.3.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.9.5)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.5.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.4.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.4.4)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (3.3.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.5.1)\n",
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.7/dist-packages (1.3.1)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.7/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.7/dist-packages (from PyDrive) (1.12.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.7.2)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=4.0.0->PyDrive) (1.15.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.17.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.8)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.26.3)\n",
            "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.30.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.23.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (20.9)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2018.9)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (3.12.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (1.53.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (56.1.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.2->PyDrive) (4.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.4.7)\n",
            "Requirement already satisfied: pandarallel in /usr/local/lib/python3.7/dist-packages (1.5.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from pandarallel) (0.3.3)\n",
            "INFO: Pandarallel will run on 4 workers.\n",
            "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd1tY4KPAkVd"
      },
      "source": [
        "Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdeaSnX8AoFd"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGB30nzSAjZb"
      },
      "source": [
        "from datasets import load_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NIQ-4faAjht"
      },
      "source": [
        "train = load_dataset('turingbench/TuringBench',name='AA', split='train')\n",
        "train = pd.DataFrame.from_dict(train)\n",
        "\n",
        "test = load_dataset('turingbench/TuringBench',name='AA', split='test')\n",
        "test = pd.DataFrame.from_dict(test)\n",
        "\n",
        "valid = load_dataset('turingbench/TuringBench',name='AA', split='valid')\n",
        "valid = pd.DataFrame.from_dict(valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWHT7xFMFvSD"
      },
      "source": [
        "The code below is used to plot the confusion matrices during training. It is adapted from [here](https://github.com/wcipriano/pretty-print-confusion-matrix)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RARNWEEDOrJP"
      },
      "source": [
        "def get_new_fig(fn, figsize=[9,9]):\n",
        "    \"\"\" Init graphics \"\"\"\n",
        "    fig1 = plt.figure(fn, figsize)\n",
        "    ax1 = fig1.gca()   #Get Current Axis\n",
        "    ax1.cla() # clear existing plot\n",
        "    return fig1, ax1\n",
        "\n",
        "def configcell_text_and_colors(array_df, lin, col, oText, facecolors, posi, fz, fmt, show_null_values=0):\n",
        "    \"\"\"\n",
        "      config cell text and colors\n",
        "      and return text elements to add and to dell\n",
        "      @TODO: use fmt\n",
        "    \"\"\"\n",
        "    text_add = []; text_del = [];\n",
        "    cell_val = array_df[lin][col]\n",
        "    tot_all = array_df[-1][-1]\n",
        "    per = (float(cell_val) / tot_all) * 100\n",
        "    curr_column = array_df[:,col]\n",
        "    ccl = len(curr_column)\n",
        "\n",
        "    #last line  and/or last column\n",
        "    if(col == (ccl - 1)) or (lin == (ccl - 1)):\n",
        "        #tots and percents\n",
        "        if(cell_val != 0):\n",
        "            if(col == ccl - 1) and (lin == ccl - 1):\n",
        "                tot_rig = 0\n",
        "                for i in range(array_df.shape[0] - 1):\n",
        "                    tot_rig += array_df[i][i]\n",
        "                per_ok = (float(tot_rig) / cell_val) * 100\n",
        "            elif(col == ccl - 1):\n",
        "                tot_rig = array_df[lin][lin]\n",
        "                per_ok = (float(tot_rig) / cell_val) * 100\n",
        "            elif(lin == ccl - 1):\n",
        "                tot_rig = array_df[col][col]\n",
        "                per_ok = (float(tot_rig) / cell_val) * 100\n",
        "            per_err = 100 - per_ok\n",
        "        else:\n",
        "            per_ok = per_err = 0\n",
        "\n",
        "        per_ok_s = ['%.2f%%'%(per_ok), '100%'] [per_ok == 100]\n",
        "\n",
        "        #text to DEL\n",
        "        text_del.append(oText)\n",
        "\n",
        "        #text to ADD\n",
        "        font_prop = fm.FontProperties(weight='bold', size=fz)\n",
        "        text_kwargs = dict(color='w', ha=\"center\", va=\"center\", gid='sum', fontproperties=font_prop)\n",
        "        lis_txt = ['%d'%(cell_val), per_ok_s, '%.2f%%'%(per_err)]\n",
        "        lis_kwa = [text_kwargs]\n",
        "        dic = text_kwargs.copy(); dic['color'] = 'g'; lis_kwa.append(dic);\n",
        "        dic = text_kwargs.copy(); dic['color'] = 'r'; lis_kwa.append(dic);\n",
        "        lis_pos = [(oText._x, oText._y-0.3), (oText._x, oText._y), (oText._x, oText._y+0.3)]\n",
        "        for i in range(len(lis_txt)):\n",
        "            newText = dict(x=lis_pos[i][0], y=lis_pos[i][1], text=lis_txt[i], kw=lis_kwa[i])\n",
        "            #print 'lin: %s, col: %s, newText: %s' %(lin, col, newText)\n",
        "            text_add.append(newText)\n",
        "        #print '\\n'\n",
        "\n",
        "        #set background color for sum cells (last line and last column)\n",
        "        carr = [0.27, 0.30, 0.27, 1.0]\n",
        "        if(col == ccl - 1) and (lin == ccl - 1):\n",
        "            carr = [0.17, 0.20, 0.17, 1.0]\n",
        "        facecolors[posi] = carr\n",
        "\n",
        "    else:\n",
        "        if(per > 0):\n",
        "            txt = '%s\\n%.2f%%' %(cell_val, per)\n",
        "        else:\n",
        "            if(show_null_values == 0):\n",
        "                txt = ''\n",
        "            elif(show_null_values == 1):\n",
        "                txt = '0'\n",
        "            else:\n",
        "                txt = '0\\n0.0%'\n",
        "        oText.set_text(txt)\n",
        "\n",
        "        #main diagonal\n",
        "        if(col == lin):\n",
        "            #set color of the textin the diagonal to white\n",
        "            oText.set_color('w')\n",
        "            # set background color in the diagonal to blue\n",
        "            facecolors[posi] = [0.35, 0.8, 0.55, 1.0]\n",
        "        else:\n",
        "            oText.set_color('r')\n",
        "\n",
        "    return text_add, text_del\n",
        "\n",
        "def insert_totals(df_cm):\n",
        "    \"\"\" insert total column and line (the last ones) \"\"\"\n",
        "    sum_col = []\n",
        "    for c in df_cm.columns:\n",
        "        sum_col.append( df_cm[c].sum() )\n",
        "    sum_lin = []\n",
        "    for item_line in df_cm.iterrows():\n",
        "        sum_lin.append( item_line[1].sum() )\n",
        "    df_cm['sum_lin'] = sum_lin\n",
        "    sum_col.append(np.sum(sum_lin))\n",
        "    df_cm.loc['sum_col'] = sum_col\n",
        "\n",
        "def pretty_plot_confusion_matrix(df_cm, annot=True, cmap=\"Oranges\", fmt='.2f', fz=11,\n",
        "      lw=0.5, cbar=False, figsize=[8,8], show_null_values=0, pred_val_axis='y'):\n",
        "    \"\"\"\n",
        "      print conf matrix with default layout (like matlab)\n",
        "      params:\n",
        "        df_cm          dataframe (pandas) without totals\n",
        "        annot          print text in each cell\n",
        "        cmap           Oranges,Oranges_r,YlGnBu,Blues,RdBu, ... see:\n",
        "        fz             fontsize\n",
        "        lw             linewidth\n",
        "        pred_val_axis  where to show the prediction values (x or y axis)\n",
        "                        'col' or 'x': show predicted values in columns (x axis) instead lines\n",
        "                        'lin' or 'y': show predicted values in lines   (y axis)\n",
        "    \"\"\"\n",
        "    if(pred_val_axis in ('col', 'x')):\n",
        "        xlbl = 'Predicted'\n",
        "        ylbl = 'Actual'\n",
        "    else:\n",
        "        xlbl = 'Actual'\n",
        "        ylbl = 'Predicted'\n",
        "        df_cm = df_cm.T\n",
        "\n",
        "    # create \"Total\" column\n",
        "    insert_totals(df_cm)\n",
        "\n",
        "    #this is for print allways in the same window\n",
        "    fig, ax1 = get_new_fig('Conf matrix default', figsize)\n",
        "\n",
        "    #thanks for seaborn\n",
        "    ax = sn.heatmap(df_cm, annot=annot, annot_kws={\"size\": fz}, linewidths=lw, ax=ax1,\n",
        "                    cbar=cbar, cmap=cmap, linecolor='w', fmt=fmt)\n",
        "\n",
        "    #set ticklabels rotation\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation = 45, fontsize = 10)\n",
        "    ax.set_yticklabels(ax.get_yticklabels(), rotation = 25, fontsize = 10)\n",
        "\n",
        "    # Turn off all the ticks\n",
        "    for t in ax.xaxis.get_major_ticks():\n",
        "        t.tick1On = False\n",
        "        t.tick2On = False\n",
        "    for t in ax.yaxis.get_major_ticks():\n",
        "        t.tick1On = False\n",
        "        t.tick2On = False\n",
        "\n",
        "    #face colors list\n",
        "    quadmesh = ax.findobj(QuadMesh)[0]\n",
        "    facecolors = quadmesh.get_facecolors()\n",
        "\n",
        "    #iter in text elements\n",
        "    array_df = np.array( df_cm.to_records(index=False).tolist() )\n",
        "    text_add = []; text_del = [];\n",
        "    posi = -1 #from left to right, bottom to top.\n",
        "    for t in ax.collections[0].axes.texts: #ax.texts:\n",
        "        pos = np.array( t.get_position()) - [0.5,0.5]\n",
        "        lin = int(pos[1]); col = int(pos[0]);\n",
        "        posi += 1\n",
        "        #print ('>>> pos: %s, posi: %s, val: %s, txt: %s' %(pos, posi, array_df[lin][col], t.get_text()))\n",
        "\n",
        "        #set text\n",
        "        txt_res = configcell_text_and_colors(array_df, lin, col, t, facecolors, posi, fz, fmt, show_null_values)\n",
        "\n",
        "        text_add.extend(txt_res[0])\n",
        "        text_del.extend(txt_res[1])\n",
        "\n",
        "    #remove the old ones\n",
        "    for item in text_del:\n",
        "        item.remove()\n",
        "    #append the new ones\n",
        "    for item in text_add:\n",
        "        ax.text(item['x'], item['y'], item['text'], **item['kw'])\n",
        "\n",
        "    #titles and legends\n",
        "    ax.set_title('Confusion matrix')\n",
        "    ax.set_xlabel(xlbl)\n",
        "    ax.set_ylabel(ylbl)\n",
        "    plt.tight_layout()  #set layout slim\n",
        "    plt.show()\n",
        "\n",
        "def plot_confusion_matrix_from_data(y_test, predictions, columns=None, annot=True, cmap=\"Oranges\",\n",
        "      fmt='.2f', fz=11, lw=0.5, cbar=False, figsize=[8,8], show_null_values=0, pred_val_axis='lin'):\n",
        "    \"\"\"\n",
        "        plot confusion matrix function with y_test (actual values) and predictions (predic),\n",
        "        whitout a confusion matrix yet\n",
        "    \"\"\"\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    from pandas import DataFrame\n",
        "\n",
        "    #data\n",
        "    if(not columns):\n",
        "        #labels axis integer:\n",
        "        ##columns = range(1, len(np.unique(y_test))+1)\n",
        "        #labels axis string:\n",
        "        from string import ascii_uppercase\n",
        "        columns = ['class %s' %(i) for i in list(ascii_uppercase)[0:len(np.unique(y_test))]]\n",
        "\n",
        "    confm = confusion_matrix(y_test, predictions)\n",
        "    cmap = 'Oranges';\n",
        "    fz = 11;\n",
        "    figsize=[9,9];\n",
        "    show_null_values = 2\n",
        "    df_cm = DataFrame(confm, index=columns, columns=columns)\n",
        "    pretty_plot_confusion_matrix(df_cm, fz=fz, cmap=cmap, figsize=figsize, show_null_values=show_null_values, pred_val_axis=pred_val_axis)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZgWqvEDhNb9"
      },
      "source": [
        "# III. Compute corpus hardness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7GsDAC_GVe_"
      },
      "source": [
        "Let us now characterize the hardness of the corpora. We implemented the Relative Hardness score, as defined here: https://dl.acm.org/doi/10.5555/1776334.1776358"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClFtvO-2hNA4"
      },
      "source": [
        "# Text pre-processing as required to compare the content of different authors\n",
        "def fil_sent(sent):\n",
        "    \"\"\"\n",
        "    Filter stopwords\n",
        "    \"\"\"\n",
        "    filtered_sentence = ' '.join([w for w in sent.split() if not w in stop_words])\n",
        "    filtered_sentence = ''.join([w for w in filtered_sentence if w not in list(punctuation)])\n",
        "    filtered_sentence = filtered_sentence.strip()\n",
        "    filtered_sentence = filtered_sentence.split()\n",
        "    return filtered_sentence\n",
        "\n",
        "def process(sent):\n",
        "    \"\"\"\n",
        "    Apply stemming\n",
        "    \"\"\"\n",
        "    sent = str(sent)\n",
        "    return fil_sent(' '.join([ps.stem(str(x).lower()) for x in word_tokenize(sent)]))\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "ps = PorterStemmer() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSCZm2_miyVe"
      },
      "source": [
        "# Similarity between two authors' content\n",
        "def jaccard_similarity(list1, list2):\n",
        "\n",
        "    list1 = list1[:512]\n",
        "    list2 = list2[:512]\n",
        "    \n",
        "    intersection = len(list(set(list1).intersection(list2)))\n",
        "    union = (len(list1) + len(list2)) - intersection\n",
        "\n",
        "    if union > 0:\n",
        "      return float(intersection) / union\n",
        "    else:\n",
        "      return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiIKx0DQjQl7"
      },
      "source": [
        "# Combines all the text per author in a list of lists\n",
        "def all_text_all_category(df, col_id, col_text):\n",
        "  all_text = []\n",
        "\n",
        "  for val in list(set(df[col_id].values)):\n",
        "    sub_df = df[df[col_id] == val]\n",
        "    all_text.append(list(itertools.chain(sub_df[col_text].apply(lambda x: process(x)).values))[0])\n",
        "\n",
        "  return all_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5EtjTmEmBEe"
      },
      "source": [
        "# Computes the relative hardness\n",
        "def rel_hardness(df, col_id, col_text):\n",
        "\n",
        "  n_labels = len(list(set(df[col_text].values)))\n",
        "  return 1/(n_labels * (n_labels - 1) * 0.5) * sum([jaccard_similarity(x[0], x[1]) for x in itertools.combinations(all_text_all_category(df, col_id, col_text), 2) if x[0] != x[1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UbMxBvRL_La"
      },
      "source": [
        "# IV. Core function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6IKCGngi7EL"
      },
      "source": [
        "The function below:\n",
        "- processes the dataframe and extracts features needed by some methods\n",
        "- runs for 5, 10, 25, 50, 75 and 100 authors:\n",
        "  - a logistic regression classifier on top of TF-IDF + Preprocessing\n",
        "  - WhoBert\n",
        "  - Style-only classifier\n",
        "  - WhoBert Style\n",
        "- returns the output performance and a graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzGDazAGO4kP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d6496c3-bca7-4c9e-bdd1-c0b280e4189e"
      },
      "source": [
        "# This is needed in case you want to try the Longtransformer for longer texts (we did not obtain better results)\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n",
        "\n",
        "# Long-sequences for Blog authorship attribution\n",
        "tokenizer = AutoTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
        "config = AutoConfig.from_pretrained('allenai/longformer-base-4096',num_labels=5)\n",
        "model_long = AutoModelForSequenceClassification.from_pretrained('allenai/longformer-base-4096',config=config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbgFON6PeYJn"
      },
      "source": [
        "On Enron, the following function takes ± 6 hours to run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpnDZkFg3D8Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8128d6fa-e36f-44d7-befb-b5056a03d258"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci5LcDwa7DWM"
      },
      "source": [
        "from sklearn.metrics import classification_report as cr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyqIllgHBu5e"
      },
      "source": [
        "def run_iterations(source=\"enron\", recompute = False):\n",
        "\n",
        "  # Load data and remove emails containing the sender's name\n",
        "\n",
        "  print(\"Loading and processing dataframe\")\n",
        "\n",
        "  ## Pre-processing functions\n",
        "  def is_name_in_email(name, email):\n",
        "    \"\"\"\n",
        "    Removing emails from Enron where name is in email\n",
        "    \"\"\"\n",
        "\n",
        "    if str(name).lower() in str(email).lower():\n",
        "      return 1\n",
        "    else:\n",
        "      return 0\n",
        "\n",
        "  def fil_sent(sent):\n",
        "      \"\"\"\n",
        "      Filter stopwords\n",
        "      \"\"\"\n",
        "\n",
        "      filtered_sentence = ' '.join([w for w in sent.split() if not w in stop_words])\n",
        "      return filtered_sentence\n",
        "\n",
        "  def process(sent):\n",
        "      \"\"\"\n",
        "      Apply stemming\n",
        "      \"\"\"\n",
        "\n",
        "      sent = str(sent)\n",
        "      return fil_sent(' '.join([ps.stem(str(x).lower()) for x in word_tokenize(sent)]))\n",
        "\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  ps = PorterStemmer() \n",
        "\n",
        "  def extract_style(text):\n",
        "    \"\"\"\n",
        "    Extracting stylometric features of a text\n",
        "    \"\"\"\n",
        "\n",
        "    text = str(text)\n",
        "    len_text = len(text)\n",
        "    len_words = len(text.split())\n",
        "    avg_len = np.mean([len(t) for t in text.split()])\n",
        "    num_short_w = len([t for t in text.split() if len(t) < 3])\n",
        "    per_digit = sum(t.isdigit() for t in text)/len(text)\n",
        "    per_cap = sum(1 for t in text if t.isupper())/len(text)\n",
        "    f_a = sum(1 for t in text if t.lower() == \"a\")/len(text)\n",
        "    f_b = sum(1 for t in text if t.lower() == \"b\")/len(text)\n",
        "    f_c = sum(1 for t in text if t.lower() == \"c\")/len(text)\n",
        "    f_d = sum(1 for t in text if t.lower() == \"d\")/len(text)\n",
        "    f_e = sum(1 for t in text if t.lower() == \"e\")/len(text)\n",
        "    f_f = sum(1 for t in text if t.lower() == \"f\")/len(text)\n",
        "    f_g = sum(1 for t in text if t.lower() == \"g\")/len(text)\n",
        "    f_h = sum(1 for t in text if t.lower() == \"h\")/len(text)\n",
        "    f_i = sum(1 for t in text if t.lower() == \"i\")/len(text)\n",
        "    f_j = sum(1 for t in text if t.lower() == \"j\")/len(text)\n",
        "    f_k = sum(1 for t in text if t.lower() == \"k\")/len(text)\n",
        "    f_l = sum(1 for t in text if t.lower() == \"l\")/len(text)\n",
        "    f_m = sum(1 for t in text if t.lower() == \"m\")/len(text)\n",
        "    f_n = sum(1 for t in text if t.lower() == \"n\")/len(text)\n",
        "    f_o = sum(1 for t in text if t.lower() == \"o\")/len(text)\n",
        "    f_p = sum(1 for t in text if t.lower() == \"p\")/len(text)\n",
        "    f_q = sum(1 for t in text if t.lower() == \"q\")/len(text)\n",
        "    f_r = sum(1 for t in text if t.lower() == \"r\")/len(text)\n",
        "    f_s = sum(1 for t in text if t.lower() == \"s\")/len(text)\n",
        "    f_t = sum(1 for t in text if t.lower() == \"t\")/len(text)\n",
        "    f_u = sum(1 for t in text if t.lower() == \"u\")/len(text)\n",
        "    f_v = sum(1 for t in text if t.lower() == \"v\")/len(text)\n",
        "    f_w = sum(1 for t in text if t.lower() == \"w\")/len(text)\n",
        "    f_x = sum(1 for t in text if t.lower() == \"x\")/len(text)\n",
        "    f_y = sum(1 for t in text if t.lower() == \"y\")/len(text)\n",
        "    f_z = sum(1 for t in text if t.lower() == \"z\")/len(text)\n",
        "    f_1 = sum(1 for t in text if t.lower() == \"1\")/len(text)\n",
        "    f_2 = sum(1 for t in text if t.lower() == \"2\")/len(text)\n",
        "    f_3 = sum(1 for t in text if t.lower() == \"3\")/len(text)\n",
        "    f_4 = sum(1 for t in text if t.lower() == \"4\")/len(text)\n",
        "    f_5 = sum(1 for t in text if t.lower() == \"5\")/len(text)\n",
        "    f_6 = sum(1 for t in text if t.lower() == \"6\")/len(text)\n",
        "    f_7 = sum(1 for t in text if t.lower() == \"7\")/len(text)\n",
        "    f_8 = sum(1 for t in text if t.lower() == \"8\")/len(text)\n",
        "    f_9 = sum(1 for t in text if t.lower() == \"9\")/len(text)\n",
        "    f_0 = sum(1 for t in text if t.lower() == \"0\")/len(text)\n",
        "    f_e_0 = sum(1 for t in text if t.lower() == \"!\")/len(text)\n",
        "    f_e_1 = sum(1 for t in text if t.lower() == \"-\")/len(text)\n",
        "    f_e_2 = sum(1 for t in text if t.lower() == \":\")/len(text)\n",
        "    f_e_3 = sum(1 for t in text if t.lower() == \"?\")/len(text)\n",
        "    f_e_4 = sum(1 for t in text if t.lower() == \".\")/len(text)\n",
        "    f_e_5 = sum(1 for t in text if t.lower() == \",\")/len(text)\n",
        "    f_e_6 = sum(1 for t in text if t.lower() == \";\")/len(text)\n",
        "    f_e_7 = sum(1 for t in text if t.lower() == \"'\")/len(text)\n",
        "    f_e_8 = sum(1 for t in text if t.lower() == \"/\")/len(text)\n",
        "    f_e_9 = sum(1 for t in text if t.lower() == \"(\")/len(text)\n",
        "    f_e_10 = sum(1 for t in text if t.lower() == \")\")/len(text)\n",
        "    f_e_11 = sum(1 for t in text if t.lower() == \"&\")/len(text)\n",
        "    richness = len(list(set(text.split())))/len(text.split())\n",
        "\n",
        "    return pd.Series([avg_len, len_text, len_words, num_short_w, per_digit, per_cap, f_a, f_b, f_c, f_d, f_e, f_f, f_g, f_h, f_i, f_j, f_k, f_l, f_m, f_n, f_o, f_p, f_q, f_r, f_s, f_t, f_u, f_v, f_w, f_x, f_y, f_z, f_0, f_1, f_2, f_3, f_4, f_5, f_6, f_7, f_8, f_9, f_e_0, f_e_1, f_e_2, f_e_3, f_e_4, f_e_5, f_e_6, f_e_7, f_e_8, f_e_9, f_e_10, f_e_11, richness])\n",
        "\n",
        "  list_senders = [20, 5] #[5, 10, 25, 50, 75, 100]\n",
        "\n",
        "  if source == \"enron\":\n",
        "\n",
        "    if recompute:\n",
        "      df = pd.read_csv('enron.csv')\n",
        "      df['name'] = df['From'].apply(lambda x: x.split(\"'\")[1].split(\".\")[0])\n",
        "      df['name_in_mail'] = df.apply(lambda x: is_name_in_email(x['name'], x['content']), axis=1)\n",
        "      df = df[df['name_in_mail'] == 0]\n",
        "      df = df[df['content'].apply(lambda x: '-----' not in str(x))]\n",
        "      df = df[df['content'].apply(lambda x: \"@\" not in str(x))]\n",
        "      df = df[df['content'].apply(lambda x: \"From: \" not in str(x))]\n",
        "      df = df[df['content'].apply(lambda x: len(str(x).split()) > 10)]\n",
        "      df['content_tfidf'] = df['content'].parallel_apply(lambda x: process(x))\n",
        "      df[[\"avg_len\", \"len_text\", \"len_words\", \"num_short_w\", \"per_digit\", \"per_cap\", \"f_a\", \"f_b\", \"f_c\", \"f_d\", \"f_e\", \"f_f\", \"f_g\", \"f_h\", \"f_i\", \"f_j\", \"f_k\", \"f_l\", \"f_m\", \"f_n\", \"f_o\", \"f_p\", \"f_q\", \"f_r\", \"f_s\", \"f_t\", \"f_u\", \"f_v\", \"f_w\", \"f_x\", \"f_y\", \"f_z\", \"f_0\", \"f_1\", \"f_2\", \"f_3\", \"f_4\", \"f_5\", \"f_6\", \"f_7\", \"f_8\", \"f_9\",  \"f_e_0\", \"f_e_1\", \"f_e_2\", \"f_e_3\", \"f_e_4\", \"f_e_5\", \"f_e_6\", \"f_e_7\", \"f_e_8\", \"f_e_9\", \"f_e_10\", \"f_e_11\", \"richness\"]] = df['content'].parallel_apply(lambda x: extract_style(x))\n",
        "      df.to_csv(\"full_enron.csv\")\n",
        "    else:\n",
        "      df = pd.read_csv('full_enron.csv')\n",
        "      df['name'] = df['From'].apply(lambda x: x.split(\"'\")[1].split(\".\")[0])\n",
        "      df['name_in_mail'] = df.apply(lambda x: is_name_in_email(x['name'], x['content']), axis=1)\n",
        "      df = df[df['name_in_mail'] == 0]\n",
        "      df = df[df['content'].apply(lambda x: '-----' not in str(x))]\n",
        "      df = df[df['content'].apply(lambda x: \"@\" not in str(x))]\n",
        "      df = df[df['content'].apply(lambda x: \"From: \" not in str(x))]\n",
        "      df = df[df['content'].apply(lambda x: len(str(x).split()) > 10)]\n",
        "      df.to_csv(\"full_enron2.csv\")\n",
        "      print(df.shape)\n",
        "\n",
        "  elif source == \"imdb\":\n",
        "\n",
        "    if recompute:\n",
        "      df = pd.read_csv('full_imdb.csv', index_col = 0)\n",
        "      df['content_tfidf'] = df['content'].parallel_apply(lambda x: process(x))\n",
        "      df[[\"avg_len\", \"len_text\", \"len_words\", \"num_short_w\", \"per_digit\", \"per_cap\", \"f_a\", \"f_b\", \"f_c\", \"f_d\", \"f_e\", \"f_f\", \"f_g\", \"f_h\", \"f_i\", \"f_j\", \"f_k\", \"f_l\", \"f_m\", \"f_n\", \"f_o\", \"f_p\", \"f_q\", \"f_r\", \"f_s\", \"f_t\", \"f_u\", \"f_v\", \"f_w\", \"f_x\", \"f_y\", \"f_z\", \"f_0\", \"f_1\", \"f_2\", \"f_3\", \"f_4\", \"f_5\", \"f_6\", \"f_7\", \"f_8\", \"f_9\",  \"f_e_0\", \"f_e_1\", \"f_e_2\", \"f_e_3\", \"f_e_4\", \"f_e_5\", \"f_e_6\", \"f_e_7\", \"f_e_8\", \"f_e_9\", \"f_e_10\", \"f_e_11\", \"richness\"]] = df['content'].parallel_apply(lambda x: extract_style(x))\n",
        "      df.to_csv(\"full_imdb_feat.csv\")\n",
        "    else:\n",
        "      df = pd.read_csv('full_imdb_feat.csv', index_col = 0)\n",
        "\n",
        "  elif source == \"imdb62\":\n",
        "    \n",
        "    list_senders = [62]\n",
        "\n",
        "    if recompute:\n",
        "\n",
        "      df = pd.read_csv(\"imdb62.txt\", sep=\"\\t\", header = None)\n",
        "      df.columns = [\"reviewid\", \"From\", \"Itemid\", \"grade\", \"title\", \"content\"]\n",
        "      df['content_tfidf'] = df['content'].parallel_apply(lambda x: process(x))\n",
        "      df[[\"avg_len\", \"len_text\", \"len_words\", \"num_short_w\", \"per_digit\", \"per_cap\", \"f_a\", \"f_b\", \"f_c\", \"f_d\", \"f_e\", \"f_f\", \"f_g\", \"f_h\", \"f_i\", \"f_j\", \"f_k\", \"f_l\", \"f_m\", \"f_n\", \"f_o\", \"f_p\", \"f_q\", \"f_r\", \"f_s\", \"f_t\", \"f_u\", \"f_v\", \"f_w\", \"f_x\", \"f_y\", \"f_z\", \"f_0\", \"f_1\", \"f_2\", \"f_3\", \"f_4\", \"f_5\", \"f_6\", \"f_7\", \"f_8\", \"f_9\",  \"f_e_0\", \"f_e_1\", \"f_e_2\", \"f_e_3\", \"f_e_4\", \"f_e_5\", \"f_e_6\", \"f_e_7\", \"f_e_8\", \"f_e_9\", \"f_e_10\", \"f_e_11\", \"richness\"]] = df['content'].parallel_apply(lambda x: extract_style(x))\n",
        "      df.to_csv(\"full_imdb62.csv\")\n",
        "\n",
        "    else:\n",
        "      \n",
        "      df = pd.read_csv('full_imdb62.csv', index_col = 0)\n",
        "\n",
        "  elif source == \"blog\":\n",
        "\n",
        "    if recompute:\n",
        "      df = pd.read_csv('/content/drive/MyDrive/Data/blogtext.csv')\n",
        "      df.columns = [\"From\", \"Gender\", \"Age\", \"Topic\", \"Sign\", \"Date\", \"content\"]\n",
        "      df = df[df['content'].apply(lambda x: len(x.split())) > 0]\n",
        "      df['content_tfidf'] = df['content'].parallel_apply(lambda x: process(x))\n",
        "      df[[\"avg_len\", \"len_text\", \"len_words\", \"num_short_w\", \"per_digit\", \"per_cap\", \"f_a\", \"f_b\", \"f_c\", \n",
        "          \"f_d\", \"f_e\", \"f_f\", \"f_g\", \"f_h\", \"f_i\", \"f_j\", \"f_k\", \"f_l\", \"f_m\", \"f_n\", \"f_o\", \"f_p\", \"f_q\", \n",
        "          \"f_r\", \"f_s\", \"f_t\", \"f_u\", \"f_v\", \"f_w\", \"f_x\", \"f_y\", \"f_z\", \"f_0\", \"f_1\", \"f_2\", \"f_3\", \"f_4\", \n",
        "          \"f_5\", \"f_6\", \"f_7\", \"f_8\", \"f_9\",  \"f_e_0\", \"f_e_1\", \"f_e_2\", \"f_e_3\", \"f_e_4\", \"f_e_5\", \"f_e_6\", \n",
        "          \"f_e_7\", \"f_e_8\", \"f_e_9\", \"f_e_10\", \"f_e_11\", \"richness\"]] = df['content'].parallel_apply(lambda x: extract_style(x))\n",
        "      df.to_csv(\"full_blog.csv\")\n",
        "    else:\n",
        "      df = pd.read_csv('full_blog.csv')\n",
        "      # Keep longer texts\n",
        "      #df = df[df['content'].apply(lambda x: len(str(x).split()) > 30)]\n",
        "\n",
        "  elif source == \"AA\":\n",
        "\n",
        "    if recompute:\n",
        "      df = pd.read_csv('/content/drive/MyDrive/TuringBench_Data/newdata.csv')\n",
        "      df = df.rename(columns={'Generation': 'content', 'label': 'From'})\n",
        "      df = df[['content', 'From']]\n",
        "      df.columns = [\"content\", \"From\"]\n",
        "      # df = df.sample(10)\n",
        "      df = df[df['content'].apply(lambda x: len(str(x).split())) > 0]\n",
        "      df['content_tfidf'] = df['content'].parallel_apply(lambda x: process(str(x)))\n",
        "      df[[\"avg_len\", \"len_text\", \"len_words\", \"num_short_w\", \"per_digit\", \"per_cap\", \"f_a\", \"f_b\", \"f_c\", \"f_d\", \n",
        "          \"f_e\", \"f_f\", \"f_g\", \"f_h\", \"f_i\", \"f_j\", \"f_k\", \"f_l\", \"f_m\", \"f_n\", \"f_o\", \"f_p\", \"f_q\", \"f_r\", \n",
        "          \"f_s\", \"f_t\", \"f_u\", \"f_v\", \"f_w\", \"f_x\", \"f_y\", \"f_z\", \"f_0\", \"f_1\", \"f_2\", \"f_3\", \"f_4\", \"f_5\", \"f_6\", \n",
        "          \"f_7\", \"f_8\", \"f_9\",  \"f_e_0\", \"f_e_1\", \"f_e_2\", \"f_e_3\", \"f_e_4\", \"f_e_5\", \"f_e_6\", \"f_e_7\", \"f_e_8\", \n",
        "          \"f_e_9\", \"f_e_10\", \"f_e_11\", \"richness\"]] = df['content'].parallel_apply(lambda x: extract_style(str(x)))\n",
        "      df.to_csv(\"/content/drive/MyDrive/TuringBench_Data/full_AA.csv\")  # CHANGE LINK TO WHERE YOU WANT TO SAVE DATA\n",
        "    else:\n",
        "      df = pd.read_csv('/content/drive/MyDrive/TuringBench_Data/full_AA.csv')\n",
        "      df = df.sample(frac=1)\n",
        "\n",
        "  list_scores = []\n",
        "  list_f1 = []\n",
        "\n",
        "  import nltk\n",
        "  from nltk.util import ngrams\n",
        "  from collections import Counter\n",
        "  import heapq\n",
        "\n",
        "  def return_best_bi_grams(text):\n",
        "      bigrams = ngrams(text,2)\n",
        "\n",
        "      data = dict(Counter(bigrams))\n",
        "      list_ngrams = heapq.nlargest(100, data.keys(), key=lambda k: data[k])\n",
        "      return list_ngrams\n",
        "\n",
        "  def return_best_tri_grams(text):\n",
        "      trigrams = ngrams(text,3)\n",
        "\n",
        "      data = dict(Counter(trigrams))\n",
        "      list_ngrams = heapq.nlargest(100, data.keys(), key=lambda k: data[k])\n",
        "      return list_ngrams\n",
        "\n",
        "  def find_freq_n_gram_in_txt(text, list_bigram, list_trigram):\n",
        "      \n",
        "      to_ret = []\n",
        "      \n",
        "      num_bigrams = len(Counter(zip(text,text[1:])))\n",
        "      num_trigrams = len(Counter(zip(text,text[1:],text[2:])))\n",
        "\n",
        "      for n_gram in list_bigram: \n",
        "          to_ret.append(text.count(''.join(n_gram))/num_bigrams)\n",
        "\n",
        "      for n_gram in list_trigram: \n",
        "          to_ret.append(text.count(''.join(n_gram))/num_trigrams)\n",
        "\n",
        "      return to_ret\n",
        "\n",
        "  print('list_senders: ', list_senders)\n",
        "\n",
        "  for limit in list_senders:\n",
        "\n",
        "    print(\"Number of speakers : \", limit)\n",
        "\n",
        "    # Select top N senders and build Train and Test\n",
        "\n",
        "    list_spk = list(pd.DataFrame(train['From'].value_counts()[:limit]).reset_index()['index'])\n",
        "    sub_df = train[train['From'].isin(list_spk)]\n",
        "    sub_df = sub_df[['From', 'content',  'content_tfidf', \"avg_len\", \"len_text\", \"len_words\", \"num_short_w\", \"per_digit\", \n",
        "                     \"per_cap\", \"f_a\", \"f_b\", \"f_c\", \"f_d\", \"f_e\", \"f_f\", \"f_g\", \"f_h\", \"f_i\", \"f_j\", \"f_k\", \"f_l\", \"f_m\", \n",
        "                     \"f_n\", \"f_o\", \"f_p\", \"f_q\", \"f_r\", \"f_s\", \"f_t\", \"f_u\", \"f_v\", \"f_w\", \"f_x\", \"f_y\", \"f_z\", \"f_0\", \n",
        "                     \"f_1\", \"f_2\", \"f_3\", \"f_4\", \"f_5\", \"f_6\", \"f_7\", \"f_8\", \"f_9\", \"f_e_0\", \"f_e_1\", \"f_e_2\", \"f_e_3\", \n",
        "                     \"f_e_4\", \"f_e_5\", \"f_e_6\", \"f_e_7\", \"f_e_8\", \"f_e_9\", \"f_e_10\", \"f_e_11\", \"richness\"]]\n",
        "    sub_df = sub_df.dropna()\n",
        "    \n",
        "    text = \" \".join(sub_df['content'].values)\n",
        "    list_bigram = return_best_bi_grams(text)\n",
        "    list_trigram = return_best_tri_grams(text)\n",
        "\n",
        "    \n",
        "    sub_df_test = test[test['From'].isin(list_spk)]\n",
        "    sub_df_test = sub_df_test[['From', 'content',  'content_tfidf', \"avg_len\", \"len_text\", \"len_words\", \"num_short_w\", \"per_digit\", \n",
        "                                \"per_cap\", \"f_a\", \"f_b\", \"f_c\", \"f_d\", \"f_e\", \"f_f\", \"f_g\", \"f_h\", \"f_i\", \"f_j\", \"f_k\", \"f_l\", \"f_m\", \n",
        "                                \"f_n\", \"f_o\", \"f_p\", \"f_q\", \"f_r\", \"f_s\", \"f_t\", \"f_u\", \"f_v\", \"f_w\", \"f_x\", \"f_y\", \"f_z\", \"f_0\", \n",
        "                                \"f_1\", \"f_2\", \"f_3\", \"f_4\", \"f_5\", \"f_6\", \"f_7\", \"f_8\", \"f_9\", \"f_e_0\", \"f_e_1\", \"f_e_2\", \"f_e_3\", \n",
        "                                \"f_e_4\", \"f_e_5\", \"f_e_6\", \"f_e_7\", \"f_e_8\", \"f_e_9\", \"f_e_10\", \"f_e_11\", \"richness\"]]\n",
        "    sub_df_test = sub_df_test.dropna()\n",
        "    \n",
        "    text1 = \" \".join(sub_df_test['content'].values)\n",
        "    list_bigram1 = return_best_bi_grams(text1)\n",
        "    list_trigram1 = return_best_tri_grams(text1)\n",
        "\n",
        "\n",
        "    print(\"Number of texts : \", len(sub_df))\n",
        "\n",
        "    dict_nlp_enron = {}\n",
        "    k=0\n",
        "\n",
        "    for val in np.unique(sub_df.From):\n",
        "        dict_nlp_enron[val] = k\n",
        "        k += 1\n",
        "\n",
        "    sub_df['Target'] = sub_df['From'].apply(lambda x: dict_nlp_enron[x])\n",
        "\n",
        "    dict_nlp_enron = {}\n",
        "    k=0\n",
        "    for val in np.unique(sub_df_test.From):\n",
        "        dict_nlp_enron[val] = k\n",
        "        k += 1\n",
        "    \n",
        "    sub_df_test['Target'] = sub_df_test['From'].apply(lambda x: dict_nlp_enron[x])\n",
        "\n",
        "\n",
        "    # ind = train_test_split(sub_df[['content', 'Target']], test_size=0.3, stratify=sub_df['Target'])\n",
        "    # ind_train = list(ind[0].index)\n",
        "    # ind_test = list(ind[1].index)\n",
        "\n",
        "    # nlp_train = sub_df.loc[ind_train]\n",
        "    # nlp_test = sub_df.loc[ind_test]\n",
        "\n",
        "    nlp_train = sub_df\n",
        "    nlp_test = sub_df_test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # TF-IDF + LR\n",
        "\n",
        "    print(\"#####\")\n",
        "    print(\"Training TF-IDF\")\n",
        "\n",
        "    vectorizer = TfidfVectorizer() #ngram_range=(1,2), max_features=3000\n",
        "    X_train = vectorizer.fit_transform(nlp_train['content_tfidf'])\n",
        "    X_test =  vectorizer.transform(nlp_test['content_tfidf'])\n",
        "\n",
        "    clf = LogisticRegression(random_state=0).fit(X_train, nlp_train['Target'])\n",
        "    y_pred = clf.predict(X_test)\n",
        "    score_lr = accuracy_score(nlp_test['Target'], y_pred)\n",
        "    f1_lr = f1_score(nlp_test['Target'], y_pred, average=\"macro\")\n",
        "    report = cr(y_true=nlp_test['Target'], y_pred=y_pred, digits=4)\n",
        "    \n",
        "    if limit < 11:\n",
        "      plot_confusion_matrix_from_data(nlp_test['Target'], y_pred)\n",
        "\n",
        "    print(\"Training done, accuracy is : \", score_lr)\n",
        "    print(\"Training done, f1-score is : \", f1_lr)\n",
        "    print('classification report: \\n', report)\n",
        "\n",
        "    # Bert + Classification Layer\n",
        "\n",
        "    print(\"#####\")\n",
        "    print(\"Training BERT\")\n",
        "\n",
        "    #if source == \"blog\":\n",
        "    #model = ClassificationModel('bert', 'bert-base-cased', limit, config, model_long, tokenizer, args={'reprocess_input_data': True, 'overwrite_output_dir': True}, use_cuda=True) \n",
        "    #else:\n",
        "    model = ClassificationModel('bert', 'bert-base-cased', num_labels=limit, args={'reprocess_input_data': True, 'overwrite_output_dir': True,  'num_train_epochs' : 5}, use_cuda=True) \n",
        "    model.train_model(nlp_train[['content', 'Target']])\n",
        "\n",
        "    predictions, raw_outputs = model.predict(list(nlp_test['content']))\n",
        "    score_bert = accuracy_score(predictions, nlp_test['Target'])\n",
        "    f1_bert = f1_score(predictions, nlp_test['Target'], average=\"macro\")\n",
        "    report = cr(y_true=nlp_test['Target'], y_pred=predictions, digits=4)\n",
        "\n",
        "\n",
        "    if limit < 11:\n",
        "      plot_confusion_matrix_from_data(nlp_test['Target'], predictions)\n",
        "\n",
        "    predictions, raw_out_train = model.predict(list(nlp_train['content']))\n",
        "    \n",
        "    print(\"Training done, accuracy is : \", score_bert)\n",
        "    print(\"Training done, f1-score is : \", f1_bert)\n",
        "    print('classification report: \\n', report)\n",
        "\n",
        "    # Style-based classifier\n",
        "\n",
        "    print(\"#####\")\n",
        "    print(\"Training style classifier\")\n",
        "\n",
        "    X_style_train = nlp_train[[\"avg_len\", \"num_short_w\", \"per_digit\", \"per_cap\", \"f_a\", \"f_b\", \"f_c\", \"f_d\", \"f_e\", \"f_f\", \"f_g\", \"f_h\", \"f_i\", \"f_j\", \n",
        "                               \"f_k\", \"f_l\", \"f_m\", \"f_n\", \"f_o\", \"f_p\", \"f_q\", \"f_r\", \"f_s\", \"f_t\", \"f_u\", \"f_v\", \"f_w\", \"f_x\", \"f_y\", \"f_z\", \"f_0\", \n",
        "                               \"f_1\", \"f_2\", \"f_3\", \"f_4\", \"f_5\", \"f_6\", \"f_7\", \"f_8\", \"f_9\", \"f_e_0\", \"f_e_1\", \"f_e_2\", \"f_e_3\", \"f_e_4\", \"f_e_5\", \n",
        "                               \"f_e_6\", \"f_e_7\", \"f_e_8\", \"f_e_9\", \"f_e_10\", \"f_e_11\", \"richness\"]]\n",
        "    X_style_test = nlp_test[[\"avg_len\", \"num_short_w\", \"per_digit\", \"per_cap\", \"f_a\", \"f_b\", \"f_c\", \"f_d\", \"f_e\", \"f_f\", \"f_g\", \"f_h\", \"f_i\", \"f_j\", \n",
        "                             \"f_k\", \"f_l\", \"f_m\", \"f_n\", \"f_o\", \"f_p\", \"f_q\", \"f_r\", \"f_s\", \"f_t\", \"f_u\", \"f_v\", \"f_w\", \"f_x\", \"f_y\", \"f_z\", \"f_0\", \n",
        "                             \"f_1\", \"f_2\", \"f_3\", \"f_4\", \"f_5\", \"f_6\", \"f_7\", \"f_8\", \"f_9\", \"f_e_0\", \"f_e_1\", \"f_e_2\", \"f_e_3\", \"f_e_4\", \"f_e_5\", \n",
        "                             \"f_e_6\", \"f_e_7\", \"f_e_8\", \"f_e_9\", \"f_e_10\", \"f_e_11\", \"richness\"]]\n",
        "    \n",
        "    #clf = xgb.XGBClassifier().fit(X_style_train, nlp_train['Target'])\n",
        "    clf = LogisticRegression(random_state=0).fit(X_style_train, nlp_train['Target'])\n",
        "    y_pred = clf.predict(X_style_test)\n",
        "    y_proba = clf.predict_proba(X_style_test)\n",
        "    y_proba_train = clf.predict_proba(X_style_train)\n",
        "    score_style = accuracy_score(nlp_test['Target'], y_pred)\n",
        "    f1_style = f1_score(nlp_test['Target'], y_pred, average=\"macro\")\n",
        "    report = cr(y_true=nlp_test['Target'], y_pred=y_pred, digits=4)\n",
        "\n",
        "\n",
        "    print(\"Training done, accuracy is : \", score_style)\n",
        "    print(\"Training done, f1-score is : \", f1_style)\n",
        "    print('classification report: \\n', report)\n",
        "\n",
        "\n",
        "    # Model Combination\n",
        "\n",
        "    print(\"#####\")\n",
        "    print(\"Model combination\")\n",
        "\n",
        "    feat_for_BERT_LR_train = np.concatenate([raw_out_train, y_proba_train], axis=1)\n",
        "    feat_for_BERT_LR_test = np.concatenate([raw_outputs, y_proba], axis=1)\n",
        "\n",
        "    clf = LogisticRegression(random_state=0).fit(feat_for_BERT_LR_train, nlp_train['Target'])\n",
        "    #clf = xgb.XGBClassifier().fit(feat_for_BERT_LR_train, nlp_train['Target'])\n",
        "\n",
        "    y_pred = clf.predict(feat_for_BERT_LR_test)\n",
        "    score_comb = accuracy_score(nlp_test['Target'], y_pred)\n",
        "    f1_comb = f1_score(nlp_test['Target'], y_pred, average=\"macro\")\n",
        "    report = cr(y_true=nlp_test['Target'], y_pred=y_pred, digits=4)\n",
        "\n",
        "\n",
        "    print(\"Training done, accuracy is : \", score_comb)\n",
        "    print(\"Training done, f1-score is : \", f1_comb)\n",
        "    print('classification report: \\n', report)\n",
        "\n",
        "\n",
        "    if limit < 11:\n",
        "      plot_confusion_matrix_from_data(nlp_test['Target'], y_pred) \n",
        "\n",
        "\n",
        "    if limit < 11:\n",
        "      plot_confusion_matrix_from_data(nlp_test['Target'], y_pred)\n",
        "\n",
        "    # Store scores\n",
        "    list_scores.append([limit, score_lr, score_bert, score_style, score_comb])\n",
        "    list_f1.append([limit, f1_lr, f1_bert, f1_style, f1_comb])\n",
        "\n",
        "  list_scores = np.array(list_scores)\n",
        "\n",
        "  # Plot the output accuracy\n",
        "  plt.figure(figsize=(12,8))\n",
        "  plt.plot(list_scores[:,0], list_scores[:,1], label=\"TF-IDF + LR\")\n",
        "  plt.plot(list_scores[:,0], list_scores[:,2], label=\"Bert + Classification layer\")\n",
        "  plt.plot(list_scores[:,0], list_scores[:,3], label=\"Stylometric\")\n",
        "  plt.plot(list_scores[:,0], list_scores[:,4], label=\"Bert + Style\")\n",
        "  plt.title(\"Classification Accuracy depending on the number of speakers\")\n",
        "  plt.xlabel(\"Number of speakers\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  return list_scores, list_f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMwWgsRq8zls"
      },
      "source": [
        "list_scores_AA = run_iterations(source=\"AA\", recompute=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGEr0_7O-TtV"
      },
      "source": [
        "# !cp '/content/full_AA.csv' '/content/drive/MyDrive/TuringBench_Data/'"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}